{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d886f376",
   "metadata": {},
   "source": [
    "This code will take the logits from subject predictions of x runs and average them - Simple Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d05f1235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[t]\n",
      "\\centering\n",
      "\\small\n",
      "\\setlength{\\tabcolsep}{2pt}\n",
      "\\renewcommand{\\arraystretch}{1.0}\n",
      "\\caption{Subject-level performance across all channel combinations (mean (std) over splits)}\n",
      "\\label{subject_level_all_combos}\n",
      "\\begin{tabular}{c|cccc}\n",
      "\\hline\n",
      "\\hline\n",
      "CH & ACC & SENS & SPEC & MCC \\\\\n",
      "\\hline\n",
      "1 & 72.97 (0.00) & 81.86 (0.00) & 63.15 (0.00) & 0.46 (0.00) \\\\\n",
      "2 & 71.28 (0.00) & 78.12 (0.00) & 63.92 (0.00) & 0.44 (0.00) \\\\\n",
      "3 & 73.94 (0.00) & 81.07 (0.00) & 65.99 (0.00) & 0.49 (0.00) \\\\\n",
      "4 & 74.33 (0.00) & 82.49 (0.00) & 65.32 (0.00) & 0.50 (0.00) \\\\\n",
      "5 & 65.50 (0.00) & 77.27 (0.00) & 52.56 (0.00) & 0.32 (0.00) \\\\\n",
      "6 & 72.28 (0.00) & 73.97 (0.00) & 70.17 (0.00) & 0.45 (0.00) \\\\\n",
      "1-2 & 74.64 (0.00) & 82.51 (0.00) & 66.01 (0.00) & 0.50 (0.00) \\\\\n",
      "1-3 & 75.64 (0.00) & 82.99 (0.00) & 67.36 (0.00) & 0.52 (0.00) \\\\\n",
      "1-4 & 77.38 (0.00) & 85.05 (0.00) & 68.84 (0.00) & 0.55 (0.00) \\\\\n",
      "1-5 & 71.25 (0.00) & 79.81 (0.00) & 61.77 (0.00) & 0.43 (0.00) \\\\\n",
      "1-6 & 75.98 (0.00) & 78.47 (0.00) & 73.05 (0.00) & 0.52 (0.00) \\\\\n",
      "2-3 & 76.33 (0.00) & 85.65 (0.00) & 65.96 (0.00) & 0.54 (0.00) \\\\\n",
      "2-4 & 77.71 (0.00) & 87.67 (0.00) & 66.72 (0.00) & 0.56 (0.00) \\\\\n",
      "2-5 & 72.62 (0.00) & 81.85 (0.00) & 62.46 (0.00) & 0.47 (0.00) \\\\\n",
      "2-6 & 74.99 (0.00) & 79.28 (0.00) & 70.25 (0.00) & 0.51 (0.00) \\\\\n",
      "3-4 & 76.68 (0.00) & 83.69 (0.00) & 68.84 (0.00) & 0.54 (0.00) \\\\\n",
      "3-5 & 71.91 (0.00) & 80.41 (0.00) & 62.41 (0.00) & 0.45 (0.00) \\\\\n",
      "3-6 & 74.29 (0.00) & 79.70 (0.00) & 68.05 (0.00) & 0.50 (0.00) \\\\\n",
      "4-5 & 71.27 (0.00) & 78.56 (0.00) & 63.23 (0.00) & 0.43 (0.00) \\\\\n",
      "4-6 & 77.35 (0.00) & 81.09 (0.00) & 73.08 (0.00) & 0.55 (0.00) \\\\\n",
      "5-6 & 73.28 (0.00) & 77.89 (0.00) & 68.13 (0.00) & 0.47 (0.00) \\\\\n",
      "1-2-3 & 77.34 (0.00) & 86.30 (0.00) & 67.39 (0.00) & 0.56 (0.00) \\\\\n",
      "1-2-4 & 77.36 (0.00) & 86.99 (0.00) & 66.70 (0.00) & 0.55 (0.00) \\\\\n",
      "1-2-5 & 75.31 (0.00) & 85.70 (0.00) & 63.92 (0.00) & 0.52 (0.00) \\\\\n",
      "1-2-6 & 79.03 (0.00) & 85.69 (0.00) & 71.70 (0.00) & 0.59 (0.00) \\\\\n",
      "1-3-4 & 76.68 (0.00) & 84.97 (0.00) & 67.41 (0.00) & 0.54 (0.00) \\\\\n",
      "1-3-5 & 75.63 (0.00) & 83.61 (0.00) & 66.67 (0.00) & 0.52 (0.00) \\\\\n",
      "1-3-6 & 79.02 (0.00) & 84.90 (0.00) & 72.36 (0.00) & 0.59 (0.00) \\\\\n",
      "1-4-5 & 76.35 (0.00) & 83.03 (0.00) & 68.84 (0.00) & 0.53 (0.00) \\\\\n",
      "1-4-6 & 78.71 (0.00) & 86.26 (0.00) & 70.25 (0.00) & 0.58 (0.00) \\\\\n",
      "1-5-6 & 74.97 (0.00) & 79.76 (0.00) & 69.53 (0.00) & 0.51 (0.00) \\\\\n",
      "2-3-4 & 78.37 (0.00) & 88.26 (0.00) & 67.44 (0.00) & 0.57 (0.00) \\\\\n",
      "2-3-5 & 74.97 (0.00) & 85.65 (0.00) & 63.20 (0.00) & 0.52 (0.00) \\\\\n",
      "2-3-6 & 77.35 (0.00) & 83.78 (0.00) & 70.27 (0.00) & 0.56 (0.00) \\\\\n",
      "2-4-5 & 76.34 (0.00) & 86.99 (0.00) & 64.63 (0.00) & 0.54 (0.00) \\\\\n",
      "2-4-6 & 79.38 (0.00) & 87.65 (0.00) & 70.27 (0.00) & 0.59 (0.00) \\\\\n",
      "2-5-6 & 76.67 (0.00) & 84.43 (0.00) & 68.15 (0.00) & 0.55 (0.00) \\\\\n",
      "3-4-5 & 76.68 (0.00) & 85.61 (0.00) & 66.75 (0.00) & 0.54 (0.00) \\\\\n",
      "3-4-6 & 77.35 (0.00) & 84.32 (0.00) & 69.58 (0.00) & 0.55 (0.00) \\\\\n",
      "3-5-6 & 75.30 (0.00) & 83.68 (0.00) & 65.99 (0.00) & 0.52 (0.00) \\\\\n",
      "4-5-6 & 77.35 (0.00) & 84.34 (0.00) & 69.56 (0.00) & 0.55 (0.00) \\\\\n",
      "1-2-3-4 & 78.36 (0.00) & 88.86 (0.00) & 66.72 (0.00) & 0.58 (0.00) \\\\\n",
      "1-2-3-5 & 78.35 (0.00) & 87.57 (0.00) & 68.10 (0.00) & 0.58 (0.00) \\\\\n",
      "1-2-3-6 & 79.02 (0.00) & 86.28 (0.00) & 70.96 (0.00) & 0.59 (0.00) \\\\\n",
      "1-2-4-5 & 76.68 (0.00) & 86.30 (0.00) & 66.03 (0.00) & 0.55 (0.00) \\\\\n",
      "1-2-4-6 & 80.40 (0.00) & 88.24 (0.00) & 71.67 (0.00) & 0.61 (0.00) \\\\\n",
      "1-2-5-6 & 78.36 (0.00) & 85.67 (0.00) & 70.27 (0.00) & 0.58 (0.00) \\\\\n",
      "1-3-4-5 & 77.36 (0.00) & 86.90 (0.00) & 66.70 (0.00) & 0.56 (0.00) \\\\\n",
      "1-3-4-6 & 78.70 (0.00) & 85.61 (0.00) & 70.96 (0.00) & 0.58 (0.00) \\\\\n",
      "1-3-5-6 & 77.34 (0.00) & 84.28 (0.00) & 69.53 (0.00) & 0.56 (0.00) \\\\\n",
      "1-4-5-6 & 78.70 (0.00) & 85.65 (0.00) & 70.99 (0.00) & 0.58 (0.00) \\\\\n",
      "2-3-4-5 & 78.70 (0.00) & 88.88 (0.00) & 67.49 (0.00) & 0.58 (0.00) \\\\\n",
      "2-3-4-6 & 80.73 (0.00) & 88.86 (0.00) & 71.70 (0.00) & 0.62 (0.00) \\\\\n",
      "2-3-5-6 & 78.34 (0.00) & 85.63 (0.00) & 70.27 (0.00) & 0.58 (0.00) \\\\\n",
      "2-4-5-6 & 78.36 (0.00) & 85.03 (0.00) & 71.03 (0.00) & 0.57 (0.00) \\\\\n",
      "3-4-5-6 & 77.69 (0.00) & 85.63 (0.00) & 68.87 (0.00) & 0.56 (0.00) \\\\\n",
      "1-2-3-4-5 & 80.73 (0.00) & 90.82 (0.00) & 69.56 (0.00) & 0.62 (0.00) \\\\\n",
      "1-2-3-4-6 & 79.71 (0.00) & 87.57 (0.00) & 70.96 (0.00) & 0.60 (0.00) \\\\\n",
      "1-2-3-5-6 & 79.37 (0.00) & 86.32 (0.00) & 71.67 (0.00) & 0.60 (0.00) \\\\\n",
      "1-2-4-5-6 & 79.71 (0.00) & 86.92 (0.00) & 71.72 (0.00) & 0.60 (0.00) \\\\\n",
      "1-3-4-5-6 & 77.34 (0.00) & 85.57 (0.00) & 68.13 (0.00) & 0.56 (0.00) \\\\\n",
      "2-3-4-5-6 & 80.40 (0.00) & 89.55 (0.00) & 70.27 (0.00) & 0.62 (0.00) \\\\\n",
      "1-2-3-4-5-6 & 80.72 (0.00) & 89.48 (0.00) & 70.99 (0.00) & 0.62 (0.00) \\\\\n",
      "\\hline\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import itertools\n",
    "import torch\n",
    "from util.metrics import calculate_metrics\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "unet2d =0\n",
    "unet1d = 1\n",
    "unet1d_0 = 0\n",
    "unet2d_0 = 0\n",
    "OP = 0\n",
    "ENC = 0\n",
    "W2V = 0\n",
    "HUB=0\n",
    "\n",
    "# --------------------\n",
    "# Your helpers (use these EXACTLY)\n",
    "# --------------------\n",
    "def np_mean_std(xs):\n",
    "    xs = np.asarray(xs, dtype=float)\n",
    "    return float(xs.mean()), float(xs.std(ddof=1)) if len(xs) > 1 else 0.0\n",
    "\n",
    "def print_fold_stats(name, vals):\n",
    "    m, s = np_mean_std(vals)\n",
    "    # print(f\"{name}: {m:.4f} ± {s:.4f}\")\n",
    "    return m\n",
    "\n",
    "def ensure_same_subject_order(dfs, subj_col=0):\n",
    "    \"\"\"\n",
    "    Ensures all DataFrames have identical subject order and IDs.\n",
    "    Raises an error if any subject is missing or out of order.\n",
    "    \"\"\"\n",
    "    subj_lists = [df.iloc[:, subj_col].tolist() for df in dfs]\n",
    "    ref = subj_lists[0]\n",
    "\n",
    "    # Check identical length\n",
    "    for i, sl in enumerate(subj_lists):\n",
    "        if len(sl) != len(ref):\n",
    "            raise ValueError(f\"[Fold align error] Path {i} has {len(sl)} subjects, expected {len(ref)}.\")\n",
    "\n",
    "    # Check identical contents\n",
    "    for i, sl in enumerate(subj_lists):\n",
    "        if set(sl) != set(ref):\n",
    "            missing = set(ref) - set(sl)\n",
    "            extra = set(sl) - set(ref)\n",
    "            raise ValueError(\n",
    "                f\"[Fold align error] Path {i} subject mismatch:\\n\"\n",
    "                f\"Missing: {sorted(missing)}\\nExtra: {sorted(extra)}\"\n",
    "            )\n",
    "\n",
    "    # If identical sets but different order, reorder\n",
    "    aligned = []\n",
    "    for df in dfs:\n",
    "        df['_subj'] = df.iloc[:, subj_col]\n",
    "        df = df.set_index('_subj').loc[ref].reset_index(drop=True)\n",
    "        aligned.append(df)\n",
    "    return aligned\n",
    "\n",
    "# --------------------\n",
    "# Config\n",
    "# --------------------\n",
    "# splits = ['xx42', 'xx52', 'xx01', 'xx02','xx03']\n",
    "splits = ['xx3']\n",
    "seed=31\n",
    "print_paths = 0\n",
    "DECIMALS = 2\n",
    "\n",
    "def natural_key(p):\n",
    "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', p.name)]\n",
    "\n",
    "def safe_iterdir(path: Path):\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    return sorted([d for d in path.iterdir() if d.is_dir()], key=natural_key)\n",
    "\n",
    "# Build per-split path dicts (kept identical to your naming scheme)\n",
    "def build_per_split_paths(split):\n",
    "    loc = {}\n",
    "    # chan1\n",
    "    loc[\"c1_path2d\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/unetssl2d_cnn/adam_s{seed}/ver138/ch1/'\n",
    "    loc[\"c1_path2d0\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/unetssl2d_cnn/adam_s{seed}/ver139/ch1/'\n",
    "    loc[\"c1_path1d\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/unetssl_cnn/adam_s{seed}/ver1/ch1/'\n",
    "    loc[\"c1_path1d0\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/unetssl_cnn/adam_s{seed}/ver142/ch1/'\n",
    "    loc[\"c1_pathOP\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/opera_ce/adam_s{seed}/ver0/ch1/'\n",
    "    loc[\"c1_pathEN\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/encodec_cnn/adamw_s{seed}/ver56/ch1/'\n",
    "    loc[\"c1_pathW2V\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/wav2vec_cnn/adam_s{seed}/ver53/ch1/'\n",
    "    loc[\"c1_pathHUB\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/hubert_cnn/adam_s{seed}/ver0/ch1/'\n",
    "\n",
    "    # chan2\n",
    "    loc[\"c2_path2d\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/unetssl2d_cnn/adam_s{seed}/ver294/ch2/'\n",
    "    loc[\"c2_path2d0\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/unetssl2d_cnn/adam_s{seed}/ver295/ch2/'\n",
    "    loc[\"c2_path1d\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/unetssl_cnn/adam_s{seed}/ver1/ch2/'\n",
    "    loc[\"c2_path1d0\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/unetssl_cnn/adam_s{seed}/ver373/ch2/'\n",
    "    loc[\"c2_pathOP\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/opera_ce/adam_s{seed}/ver32/ch2/'\n",
    "    loc[\"c2_pathEN\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/encodec_cnn/adamw_s{seed}/ver173/ch2/'\n",
    "    loc[\"c2_pathW2V\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/wav2vec_cnn/adam_s{seed}/ver161/ch2/'\n",
    "    loc[\"c2_pathHUB\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/hubert_cnn/adam_s{seed}/ver36/ch2/'\n",
    "\n",
    "    # chan3\n",
    "    loc[\"c3_path2d\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/unetssl2d_cnn/adam_s{seed}/ver552/ch3/'\n",
    "    loc[\"c3_path2d0\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/unetssl2d_cnn/adam_s{seed}/ver553/ch3/'\n",
    "    loc[\"c3_path1d\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/unetssl_cnn/adam_s{seed}/ver1/ch3/'\n",
    "    loc[\"c3_path1d0\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/unetssl_cnn/adam_s{seed}/ver628/ch3/'\n",
    "    loc[\"c3_pathOP\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/opera_ce/adam_s{seed}/ver64/ch3/'\n",
    "    loc[\"c3_pathEN\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/encodec_cnn/adamw_s{seed}/ver334/ch3/'\n",
    "    loc[\"c3_pathW2V\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/wav2vec_cnn/adam_s{seed}/ver323/ch3/'\n",
    "    loc[\"c3_pathHUB\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/hubert_cnn/adam_s{seed}/ver69/ch3/'\n",
    "\n",
    "    # chan4\n",
    "    loc[\"c4_path2d\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/unetssl2d_cnn/adam_s{seed}/ver1028/ch4/'\n",
    "    loc[\"c4_path2d0\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/unetssl2d_cnn/adam_s{seed}/ver1029/ch4/'\n",
    "    loc[\"c4_path1d\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/unetssl_cnn/adam_s{seed}/ver1/ch4/'\n",
    "    loc[\"c4_path1d0\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/unetssl_cnn/adam_s{seed}/ver845/ch4/'\n",
    "    loc[\"c4_pathOP\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/opera_ce/adam_s{seed}/ver97/ch4/'\n",
    "    loc[\"c4_pathEN\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/encodec_cnn/adamw_s{seed}/ver479/ch4/'\n",
    "    loc[\"c4_pathW2V\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/wav2vec_cnn/adam_s{seed}/ver443/ch4/'\n",
    "    loc[\"c4_pathHUB\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/hubert_cnn/adam_s{seed}/ver100/ch4/'\n",
    "\n",
    "    # chan5\n",
    "    loc[\"c5_path2d\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/unetssl2d_cnn/adam_s{seed}/ver1152/ch5'\n",
    "    loc[\"c5_path2d0\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/unetssl2d_cnn/adam_s{seed}/ver1153/ch5'\n",
    "    loc[\"c5_path1d\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/unetssl_cnn/adam_s{seed}/ver1/ch5'\n",
    "    loc[\"c5_path1d0\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/unetssl_cnn/adam_s{seed}/ver975/ch5'\n",
    "    loc[\"c5_pathOP\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/opera_ce/adam_s{seed}/ver144/ch5/'\n",
    "    loc[\"c5_pathEN\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/encodec_cnn/adamw_s{seed}/ver632/ch5/'\n",
    "    loc[\"c5_pathW2V\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/wav2vec_cnn/adam_s{seed}/ver608/ch5/'\n",
    "    loc[\"c5_pathHUB\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/hubert_cnn/adam_s{seed}/ver130/ch5/'\n",
    "\n",
    "    # chan6\n",
    "    loc[\"c6_path2d\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/unetssl2d_cnn/adam_s{seed}/ver1374/ch6/'\n",
    "    loc[\"c6_path2d0\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/unetssl2d_cnn/adam_s{seed}/ver1375/ch6/'\n",
    "    loc[\"c6_path1d\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/unetssl_cnn/adam_s{seed}/ver1/ch6/'\n",
    "    loc[\"c6_path1d0\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/unetssl_cnn/adam_s{seed}/ver1339/ch6/'\n",
    "    loc[\"c6_pathOP\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/opera_ce/adam_s{seed}/ver160/ch6/'\n",
    "    loc[\"c6_pathEN\"]=   f'/home/sparc/dev/code/saved_results/saved{split}/encodec_cnn/adamw_s{seed}/ver749/ch6/'\n",
    "    loc[\"c6_pathW2V\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/wav2vec_cnn/adam_s{seed}/ver755/ch6/'\n",
    "    loc[\"c6_pathHUB\"]=  f'/home/sparc/dev/code/saved_results/saved{split}/hubert_cnn/adam_s{seed}/ver163/ch6/'\n",
    "\n",
    "    return loc\n",
    "\n",
    "# Generate channel combinations of size 2..6\n",
    "combos = []\n",
    "for k in range(1, 7):\n",
    "    combos.extend(itertools.combinations([1,2,3,4,5,6], k))\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for combo in combos:\n",
    "    chan_comb = list(combo)  # e.g., [1,2,3]\n",
    "    accP, senP, speP, mccP = [], [], [], []  # per-split means (each split is mean over folds)\n",
    "\n",
    "    for split in splits:\n",
    "        loc = build_per_split_paths(split)\n",
    "\n",
    "        # Build the list of model paths for this combo and flags (identical to your logic)\n",
    "        paths = []\n",
    "        for c in chan_comb:\n",
    "            if unet2d == 1:\n",
    "                paths.append(loc[f\"c{c}_path2d\"])\n",
    "            if unet1d == 1:\n",
    "                paths.append(loc[f\"c{c}_path1d\"])\n",
    "            if unet1d_0 == 1:\n",
    "                paths.append(loc[f\"c{c}_path1d0\"])\n",
    "            if unet2d_0 == 1:\n",
    "                paths.append(loc[f\"c{c}_path2d0\"])\n",
    "            if OP == 1:\n",
    "                paths.append(loc[f\"c{c}_pathOP\"])\n",
    "            if ENC == 1:\n",
    "                paths.append(loc[f\"c{c}_pathEN\"])\n",
    "            if W2V == 1:\n",
    "                paths.append(loc[f\"c{c}_pathW2V\"])\n",
    "            if HUB == 1:\n",
    "                paths.append(loc[f\"c{c}_pathHUB\"])\n",
    "\n",
    "        # Load per-path, per-fold dataframes\n",
    "        all_paths_folds = []\n",
    "        for path in paths:\n",
    "            Path_ch = Path(path)\n",
    "            fold_dfs = []\n",
    "            for d in safe_iterdir(Path_ch):\n",
    "                csv_path = os.path.join(d, 'subject_pred.csv')\n",
    "                if os.path.isfile(csv_path):\n",
    "                    try:\n",
    "                        sub_pred = pd.read_csv(csv_path)\n",
    "                        fold_dfs.append(sub_pred)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            if fold_dfs:\n",
    "                all_paths_folds.append(fold_dfs)\n",
    "\n",
    "        if not all_paths_folds:\n",
    "            continue  # no data for this split/combination\n",
    "\n",
    "        # Normalize fold counts across paths\n",
    "        num_paths = len(all_paths_folds)\n",
    "        num_folds = min(len(f) for f in all_paths_folds)\n",
    "        if not all(len(f) == num_folds for f in all_paths_folds):\n",
    "            print(f\"[WARN] {chan_comb} split {split}: varying folds; using {num_folds}\")\n",
    "\n",
    "        # Fusion per fold (same as your fusion block)\n",
    "        fusion_fold_metrics = {\"acc\": [], \"sen\": [], \"spe\": [], \"mcc\": [], \"f1p\": [], \"f1n\": []}\n",
    "        w = [1.0 / num_paths] * num_paths\n",
    "\n",
    "        for f_idx in range(num_folds):\n",
    "            fold_dfs = [all_paths_folds[p_idx][f_idx] for p_idx in range(num_paths)]\n",
    "            # IMPORTANT: use your ensure_same_subject_order\n",
    "            fold_dfs = ensure_same_subject_order(fold_dfs, subj_col=0)\n",
    "\n",
    "            labels = torch.as_tensor(fold_dfs[0].iloc[:, 1].to_numpy())\n",
    "            logits_list = [torch.as_tensor(df.iloc[:, 4].to_numpy(), dtype=torch.float32) for df in fold_dfs]\n",
    "            avg_prob = sum(w[p] * logits_list[p] for p in range(num_paths))\n",
    "            pred = avg_prob.round()  # threshold 0.5\n",
    "\n",
    "            acc, sen, spe, mcc, f1p, f1n = calculate_metrics(labels, pred)\n",
    "            fusion_fold_metrics[\"acc\"].append(acc)\n",
    "            fusion_fold_metrics[\"sen\"].append(sen)\n",
    "            fusion_fold_metrics[\"spe\"].append(spe)\n",
    "            fusion_fold_metrics[\"mcc\"].append(mcc)\n",
    "            fusion_fold_metrics[\"f1p\"].append(f1p)\n",
    "            fusion_fold_metrics[\"f1n\"].append(f1n)\n",
    "\n",
    "        # Use your print_fold_stats to get per-split means (over folds)\n",
    "        accP.append(print_fold_stats('acc', fusion_fold_metrics['acc']))\n",
    "        senP.append(print_fold_stats('sen', fusion_fold_metrics['sen']))\n",
    "        speP.append(print_fold_stats('spe', fusion_fold_metrics['spe']))\n",
    "        mccP.append(print_fold_stats('mcc', fusion_fold_metrics['mcc']))\n",
    "\n",
    "    # After all splits → compute mean(std) across splits\n",
    "    def mean_std_fmt(values, decimals=2, percent=True):\n",
    "        mu, sd = np_mean_std(values)  # uses ddof=1\n",
    "        if percent:\n",
    "            mu *= 100; sd *= 100\n",
    "        return f\"{mu:.{decimals}f} ({sd:.{decimals}f})\"\n",
    "\n",
    "    acc_str = mean_std_fmt(accP, DECIMALS, percent=True)\n",
    "    sen_str = mean_std_fmt(senP, DECIMALS, percent=True)\n",
    "    spe_str = mean_std_fmt(speP, DECIMALS, percent=True)\n",
    "    mcc_str = mean_std_fmt(mccP, DECIMALS, percent=False)\n",
    "\n",
    "    chan_str = \"-\".join(str(x) for x in chan_comb)\n",
    "    all_rows.append([chan_str, acc_str, sen_str, spe_str, mcc_str])\n",
    "\n",
    "# --------------------\n",
    "# Emit LaTeX table\n",
    "# --------------------\n",
    "caption = \"Subject-level performance across all channel combinations (mean (std) over splits)\"\n",
    "label = \"subject_level_all_combos\"\n",
    "colspec = \"c|cccc\"\n",
    "\n",
    "latex_lines = [r\"\"\"\\begin{table}[t]\n",
    "\\centering\n",
    "\\small\n",
    "\\setlength{\\tabcolsep}{2pt}\n",
    "\\renewcommand{\\arraystretch}{1.0}\n",
    "\\caption{\"\"\" + caption + r\"\"\"}\n",
    "\\label{\"\"\" + label + r\"\"\"}\n",
    "\\begin{tabular}{\"\"\" + colspec + r\"\"\"}\n",
    "\\hline\n",
    "\\hline\n",
    "CH & ACC & SENS & SPEC & MCC \\\\\n",
    "\\hline\n",
    "\"\"\"]\n",
    "\n",
    "# Sort by combination length then numerically\n",
    "all_rows_sorted = sorted(all_rows, key=lambda r: (len(r[0].split('-')), [int(x) for x in r[0].split('-')]))\n",
    "for r in all_rows_sorted:\n",
    "    latex_lines.append(\" & \".join(r) + r\" \\\\\" + \"\\n\")\n",
    "\n",
    "latex_lines.append(r\"\"\"\\hline\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\")\n",
    "\n",
    "latex_table = \"\".join(latex_lines)\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b6a443",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
